{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fedcc4c8-68e8-4456-a1df-e1da5aab2151",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "from time import perf_counter\n",
    "from typing import Optional, List, Sequence\n",
    "\n",
    "import einops\n",
    "import jax\n",
    "import gin\n",
    "import librosa\n",
    "import nest_asyncio\n",
    "import matplotlib.pyplot as plt\n",
    "import pylab\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "import gradio as gr\n",
    "from flax.core import freeze\n",
    "import seqio\n",
    "from absl import logging\n",
    "from PIL import ImageColor, ImageOps\n",
    "import cv2\n",
    "\n",
    "\n",
    "# Uncomment to log compilations for jitted funtions\n",
    "# environ[\"JAX_LOG_COMPILES\"] = \"1\"\n",
    "\n",
    "# On by default in train.py and eval.py\n",
    "jax.config.update(\"jax_parallel_functions_output_gda\", True)\n",
    "\n",
    "# Hack for https://github.com/scikit-video/scikit-video/issues/154, not sure\n",
    "# how were avoiding this issue before but this fix works me\n",
    "import numpy\n",
    "\n",
    "numpy.float = numpy.float64\n",
    "numpy.int = numpy.int_\n",
    "\n",
    "from t5x import utils\n",
    "from t5x import partitioning\n",
    "\n",
    "from t5x.examples.unified_io.data import tasks \n",
    "from t5x.examples.unified_io.data.data_utils import (\n",
    "    get_default_vocabulary,\n",
    "    resize_and_pad_default,\n",
    "    MODALITY_EXTRA_ID_N_FRAMES,\n",
    ")\n",
    "\n",
    "from t5x.examples.unified_io import modality_processing\n",
    "from t5x.examples.unified_io import utils as uio_utils\n",
    "from t5x.examples.unified_io import config\n",
    "from t5x.examples.unified_io import decoding\n",
    "from t5x.examples.unified_io.metrics.utils import (\n",
    "    extract_bboxes_from_text,\n",
    "    extract_actions_from_prediction,\n",
    "    reconstruct_vima_action,\n",
    "    extract_points_from_text,\n",
    ")\n",
    "\n",
    "import demo.utils as U\n",
    "\n",
    "logging.set_verbosity(logging.INFO)\n",
    "\n",
    "# Needed when running interactively\n",
    "nest_asyncio.apply()\n",
    "gin.enter_interactive_mode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6c81c7-8665-4b8a-9c5b-be720d6eaf18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model and parameters\n",
    "\n",
    "FULL_CKPT_PATH = \"xxl-3m\"  # Modify to point to your checkpoint\n",
    "MODEL_TYPE = \"xxl\"  # Set 'large', 'xl', or 'xxl\n",
    "\n",
    "assert FULL_CKPT_PATH is not None\n",
    "\n",
    "vocab = get_default_vocabulary()\n",
    "\n",
    "# For GPUS\n",
    "n_gpus = 1  \n",
    "supports_bfloat16 = False  # If your GPUs support this, setting to true to improve performance\n",
    "model = uio_utils.get_model(MODEL_TYPE, dtype=\"bfloat16\" if supports_bfloat16 else \"float32\")\n",
    "partitioner = partitioning.PjitPartitioner(num_partitions=n_gpus)\n",
    "\n",
    "# For a TPU:\n",
    "# model = uio_utils.get_model(MODEL_TYPE)\n",
    "# partitioner = partitioning.PjitPartitioner(num_partitions=8)\n",
    "\n",
    "parameters, param_axes = uio_utils.get_parameters(model, FULL_CKPT_PATH, partitioner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9453a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load HIFGAN, which converts the melspectorgram UIO2 outputs into audio waveforms that can be played\n",
    "USE_HIFIGAN = True\n",
    "if USE_HIFIGAN:\n",
    "    from demo.hifigan.models import Generator\n",
    "    from demo.hifigan.env import AttrDict\n",
    "    import json\n",
    "    import torch\n",
    "\n",
    "    config_file = os.path.join(\"demo/hifigan/checkpoints\", \"config.json\")\n",
    "    with open(config_file) as f:\n",
    "        data = f.read()\n",
    "\n",
    "    # global h\n",
    "    json_config = json.loads(data)\n",
    "    h = AttrDict(json_config)\n",
    "    # global torch_device\n",
    "    torch_device = torch.device(\"cpu\")\n",
    "\n",
    "    def load_checkpoint(filepath, device):\n",
    "        assert os.path.isfile(filepath)\n",
    "        print(\"Loading '{}'\".format(filepath))\n",
    "        checkpoint_dict = torch.load(filepath, map_location=device)\n",
    "        return checkpoint_dict\n",
    "\n",
    "    hifigan_generator = Generator(h).to(torch_device)\n",
    "    state_dict_g = load_checkpoint(\"demo/hifigan/checkpoints/g_00930000\", torch_device)\n",
    "    hifigan_generator.load_state_dict(state_dict_g[\"generator\"])\n",
    "\n",
    "    hifigan_generator.eval()\n",
    "    hifigan_generator.remove_weight_norm()\n",
    "    print(\"Complete.\")\n",
    "else:\n",
    "    hifigan_generator = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce050245",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define sequence_len, which determines how much to pad the inputs and how many patches to sample\n",
    "IMG_HISTORY_MAX_FRAMES = 4\n",
    "AUDIO_HISTORY_MAX_FRAMES = 4\n",
    "\n",
    "subsample_ratio = 1.0\n",
    "sequence_len = {\n",
    "    \"is_training\": False,\n",
    "    \"text_inputs\": 512,\n",
    "    \"text_targets\": 512,\n",
    "    \"image_input_samples\": int(\n",
    "        config.IMAGE_INPUT_SIZE[0] // config.IMAGE_INPUT_D * config.IMAGE_INPUT_SIZE[1] // config.IMAGE_INPUT_D * subsample_ratio\n",
    "    ),\n",
    "    \"image_history_input_samples\": int(\n",
    "        config.IMAGE_HISTORY_INPUT_SIZE[0] // config.IMAGE_HISTORY_INPUT_D\n",
    "        * config.IMAGE_HISTORY_INPUT_SIZE[1] // config.IMAGE_HISTORY_INPUT_D\n",
    "        * subsample_ratio\n",
    "    ),\n",
    "    \"audio_input_samples\": int(\n",
    "        config.AUDIO_INPUT_SIZE[0] // config.AUDIO_INPUT_D * config.AUDIO_INPUT_SIZE[1] // config.AUDIO_INPUT_D  * subsample_ratio\n",
    "    ),\n",
    "    \"audio_history_input_samples\": int(\n",
    "        config.AUDIO_HISTORY_INPUT_SIZE[0] // config.AUDIO_HISTORY_INPUT_D\n",
    "        * config.AUDIO_HISTORY_INPUT_SIZE[1] // config.AUDIO_HISTORY_INPUT_D\n",
    "        * subsample_ratio\n",
    "    ),\n",
    "    \"num_frames\": IMG_HISTORY_MAX_FRAMES,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b600f3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The main inference function, batch contains the pre-processed inputs\n",
    "def _fn(\n",
    "    params,\n",
    "    batch,\n",
    "    decoder_params,\n",
    "    return_all_decodes,\n",
    "    num_decodes,\n",
    "    modality,\n",
    "    length,\n",
    "    decode_rng,\n",
    "    top_k,\n",
    "    top_p,\n",
    "    temperature,\n",
    "    repetition_penalty,\n",
    "    horizontally_pack_inputs,\n",
    "    alpha=None,\n",
    "    greedy=True,\n",
    "    negative_prompt=None\n",
    "):\n",
    "    if greedy:\n",
    "        model._decode_fn = decoding.beam_search\n",
    "        assert negative_prompt is None\n",
    "    else:\n",
    "        model._decode_fn = decoding.temperature_sample\n",
    "    return model.predict_batch_with_aux(\n",
    "        params, batch, decoder_params, return_all_decodes, num_decodes, length, modality,\n",
    "        decode_rng=decode_rng, top_k=top_k, top_p=top_p, temperature=temperature, alpha=alpha,\n",
    "        repetition_penalty=repetition_penalty, horizontally_pack_inputs=horizontally_pack_inputs,\n",
    "        negative_prompt=negative_prompt\n",
    "    )\n",
    "\n",
    "\n",
    "# A version of the inference function that will lazily be compiled into a partitioned version when called\n",
    "_partitioned_infer_step = partitioner.partition(\n",
    "  _fn,\n",
    "  in_axis_resources=(param_axes,\n",
    "                     partitioner.data_partition_spec,\n",
    "                     None, None, None, None),\n",
    "  out_axis_resources=None,\n",
    "  # seed (arg 7) temp (arg 10) alpha (arg 13) and negative prompt (15) are left non-static so they don't require re-compilation\n",
    "  static_argnums=(2, 3, 4, 5, 6, 8, 9, 11, 12, 14)  \n",
    ")\n",
    "\n",
    "# the partitioned function no longer accepts keywords, so we build a version that does here for convenience\n",
    "def partitioned_infer_step(\n",
    "    params,\n",
    "    batch,\n",
    "    decoder_params=None,\n",
    "    return_all_decodes=False,\n",
    "    num_decodes=1,\n",
    "    length=None,\n",
    "    modality=\"text\",\n",
    "    decode_rng=None,\n",
    "    top_k=0,\n",
    "    top_p=1.0,\n",
    "    temperature=1,\n",
    "    repetition_penalty=None,\n",
    "    horizontally_pack_inputs=None,\n",
    "    alpha=None,\n",
    "    greedy=True,\n",
    "    negative_prompt=None\n",
    "):\n",
    "    return _partitioned_infer_step(\n",
    "        params, batch, decoder_params, return_all_decodes, num_decodes,\n",
    "        modality, length, decode_rng, top_k, top_p, temperature, repetition_penalty,\n",
    "        horizontally_pack_inputs, alpha, greedy, negative_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0ac0f1-4607-4ff4-aa94-4fd19304c26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UnifiedIO 2 pre-processing\n",
    "\n",
    "def build_input_dict(input_text, input_image=None, input_audio=None, audio_history=None, image_history=None):\n",
    "  out = {}\n",
    "\n",
    "  if input_image is not None:\n",
    "    image_input = tf.image.convert_image_dtype(input_image, dtype=tf.float32)\n",
    "    image_input, image_input_mask, _ = resize_and_pad_default(image_input, False)\n",
    "    out[\"image_inputs\"] = image_input\n",
    "    out[\"image_input_masks\"] = image_input_mask\n",
    "\n",
    "  if input_text is not None:\n",
    "    out[\"text_inputs\"] = input_text\n",
    "    \n",
    "  if image_history is not None and not isinstance(image_history, Sequence):\n",
    "    # Assume image history is a 4D tensor\n",
    "    assert len(image_history.shape) == 4\n",
    "    video_tensor = tf.image.convert_image_dtype(image_history, dtype=tf.float32)\n",
    "    video_tensor, video_mask, _ = resize_and_pad_default(video_tensor, False)\n",
    "    out[\"image_history_inputs\"] = video_tensor\n",
    "    out[\"image_history_input_masks\"] = video_mask\n",
    "\n",
    "  elif image_history is not None and any(x is not None for x in image_history):\n",
    "    # Image history contains a list of possibly None images\n",
    "    image_history_inputs = []\n",
    "    image_history_input_masks = []\n",
    "    for image in image_history:\n",
    "      if image is not None:\n",
    "        img = tf.image.convert_image_dtype(image, dtype=tf.float32)\n",
    "        img_input, img_input_mask, _ = resize_and_pad_default(img, False, is_history=True)\n",
    "      else:\n",
    "        img_input = tf.zeros(\n",
    "          [config.IMAGE_HISTORY_INPUT_SIZE[0], config.IMAGE_HISTORY_INPUT_SIZE[1], 3], dtype=tf.float32)\n",
    "        img_input_mask = tf.zeros(config.IMAGE_HISTORY_INPUT_SIZE, dtype=tf.int32)\n",
    "\n",
    "      image_history_inputs.append(img_input)\n",
    "      image_history_input_masks.append(img_input_mask)\n",
    "\n",
    "    out[\"image_history_inputs\"] = np.stack(image_history_inputs)\n",
    "    out[\"image_history_input_masks\"] = np.stack(image_history_input_masks)\n",
    "\n",
    "  # Audio has the same pre-processing for history and input, so we batch them\n",
    "  all_audio = []\n",
    "  if input_audio is not None:\n",
    "    all_audio.append(input_audio[None, :, :])\n",
    "\n",
    "  if audio_history is not None:\n",
    "    all_audio.append(audio_history)\n",
    "\n",
    "  if all_audio:\n",
    "    print([x.shape for x in all_audio])\n",
    "    all_audio = tf.concat(all_audio, 0)\n",
    "    all_audio = tf.transpose(all_audio, perm=[0, 2, 1])\n",
    "\n",
    "    audio_mask = tf.cast(all_audio != 0, tf.float32)\n",
    "    all_audio = tf.math.log(tf.clip_by_value(all_audio, 1e-5, 1e5))\n",
    "    all_audio = all_audio * audio_mask\n",
    "    audio_mask = tf.cast(audio_mask, tf.int32)\n",
    "    all_audio = tf.expand_dims(all_audio, -1)\n",
    "\n",
    "    if input_audio is not None:\n",
    "      out[\"audio_inputs\"] = all_audio[0]\n",
    "      out[\"audio_input_masks\"] = audio_mask[0]\n",
    "      all_audio = all_audio[1:]\n",
    "      audio_mask = audio_mask[1:]\n",
    "\n",
    "    if audio_history is not None:\n",
    "      out[\"audio_history_inputs\"] = all_audio\n",
    "      out[\"audio_history_input_masks\"] = audio_mask\n",
    "\n",
    "  return out\n",
    "\n",
    "\n",
    "def build_batch(\n",
    "  input_text, \n",
    "  input_image=None, \n",
    "  input_audio=None, \n",
    "  image_history=None, \n",
    "  audio_history=None,\n",
    "  return_resized_input_image=False\n",
    "):\n",
    "  \"\"\"Builds a size one batch of preprocessed inputs that can be passed to the model\n",
    "\n",
    "  input_text: Input string\n",
    "  input_audio: Input spectrogram\n",
    "  input_image: Input image array\n",
    "  audio_history: 3D Tensor of spectrograms\n",
    "  image_history: Either a list of images, or a 4D tensor of images\n",
    "\n",
    "  ret: A batch of pre-preprocessed data\n",
    "  \"\"\"\n",
    "  # Pre-processing functions for UIO2 are built for tf.data.Datasets, so we convert\n",
    "  # the inputs into a size one dataset and apply those functions here,\n",
    "  # and then extract a size one tf.data.Dataset from the dataset at the end\n",
    "  batch = build_input_dict(input_text, input_image, input_audio, audio_history, image_history)\n",
    "  resized_input_image = batch.get(\"image_inputs\") if return_resized_input_image else None\n",
    "  dataset = tf.data.Dataset.from_tensors(batch)\n",
    "  dataset = modality_processing.unified_io_preprocessor(\n",
    "    dataset, modality_processing.OUTPUT_FEATURES, sequence_len)\n",
    "  converter = modality_processing.UnifiedIOFeatureConverter()\n",
    "  dataset = converter(dataset, sequence_len)\n",
    "  dataset = dataset.batch(1)\n",
    "  if return_resized_input_image:\n",
    "    return next(dataset.as_numpy_iterator()), resized_input_image\n",
    "  return next(dataset.as_numpy_iterator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83333785-bd97-4787-9391-4c60d176588e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to get a prediction\n",
    "# batch = build_batch(\"[Text] [S] test prompt\", input_image=np.zeros((256, 256, 3), np.float32))\n",
    "# partitioned_infer_step(parameters, batch, None, True, 1, length=32, modality=\"text\", greedy=False)\n",
    "\n",
    "# With CLS free guidance\n",
    "# negative_prompt = uio_utils.build_batch(\"[Text] [S]\")\n",
    "# partitioned_infer_step(parameters, batch, None, True, 1, length=32, modality=\"text\", alpha=1, negative_prompt=negative_prompt, greedy=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7bd4f96-b431-4592-9b56-fb6c0322b8c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradio functionality, including loading videos and building audio spectrograms\n",
    "INPUT_LEN_BUCKETS = np.array([64, 256, 512, 1024])\n",
    "N_ROWS = 2\n",
    "N_PER_ROW = 4\n",
    "assert IMG_HISTORY_MAX_FRAMES % N_PER_ROW == 0  # for convenience\n",
    "NUM_DECODES = N_ROWS * N_PER_ROW\n",
    "\n",
    "\n",
    "DUMMY_NEGATIVE_PROMPT = build_batch(\"\")\n",
    "\n",
    "\n",
    "def load_audio(path):\n",
    "    spectrograms = U.load_audio(\n",
    "        path,\n",
    "        audio_segment_length=config.AUDIO_SEGMENT_LENGTH,\n",
    "        spectrogram_length=config.AUDIO_SEGMENT_LENGTH,\n",
    "        max_audio_length=config.AUDIO_SEGMENT_LENGTH * (AUDIO_HISTORY_MAX_FRAMES + 1),\n",
    "    )\n",
    "    return spectrograms\n",
    "\n",
    "\n",
    "def load_video(path, **kwargs):\n",
    "    if path.endswith(\".wav\"):\n",
    "        gr.Warning(f\"Input is audio file {path}\")\n",
    "        return None, load_audio(path)\n",
    "    frames, spectrograms = U.load_video(path, **kwargs)\n",
    "    return frames, spectrograms\n",
    "\n",
    "\n",
    "\n",
    "def compute_non_masked_input_tokens(example):\n",
    "  \"\"\"Compute the number of non-masked tokens in the input\"\"\"\n",
    "  encoder_len = 0\n",
    "  for key, v in modality_processing.get_input_modalities().items():\n",
    "    mask = example[f\"inputs/{key}/mask\"]\n",
    "    assert mask.shape[0] == 1\n",
    "    mask = mask[0]\n",
    "    seq_len = v.get_static_sequence_len()\n",
    "    if seq_len is not None:\n",
    "      # History encoding compress each valid frame into `seq_len` tokens\n",
    "      valid_frames = tf.reduce_any(tf.reshape(mask > 0, [mask.shape[0], -1]), -1)\n",
    "      n_frames = tf.reduce_sum(tf.cast(valid_frames, tf.int32))\n",
    "      encoder_len += n_frames * seq_len\n",
    "    else:\n",
    "      encoder_len += mask.sum()\n",
    "  return encoder_len\n",
    "\n",
    "\n",
    "def _scale_resize(image_np, desired_output_size):\n",
    "    desired_height, desired_width = desired_output_size\n",
    "    height, width = image_np.shape[:2]\n",
    "    scale_factor = min(desired_height / height, desired_width / width)\n",
    "    scaled_height = int(height * scale_factor)\n",
    "    scaled_width = int(width * scale_factor)\n",
    "    image_pil = Image.fromarray(image_np).resize((scaled_width, scaled_height), Image.BILINEAR)\n",
    "    return np.array(image_pil)\n",
    "    \n",
    "\n",
    "# Main Gradio inference functions\n",
    "def run_inference(\n",
    "    output_modality,\n",
    "    random_seed,\n",
    "    top_k,\n",
    "    top_p,\n",
    "    temperature,\n",
    "    guidance_scale,\n",
    "    repetition_penalty,\n",
    "    n_outputs,\n",
    "    negative_prompt,\n",
    "    input_decoding,\n",
    "    bbox_annotate_image,\n",
    "    input_text,\n",
    "    input_image,\n",
    "    input_video,\n",
    "    input_audio,\n",
    "    *image_histories,\n",
    "):\n",
    "    if input_text is None or input_text == \"\":\n",
    "        print(gr.Warning('No prompt specified, predictions will be random!'))\n",
    "        input_text = \"\"\n",
    "    output_modality = output_modality.lower()\n",
    "    # Prefix to automatically add to input text\n",
    "    prefix = dict(image=\"[Image] [S] \", audio=\"[Audio] [S] \", text=\"[Text] [S] \")[output_modality]\n",
    "\n",
    "    # Classifier free guidance batch if being used\n",
    "    if guidance_scale == 0:\n",
    "        negative_prompt = None\n",
    "    if negative_prompt is not None:\n",
    "        negative_prompt_batch = build_batch(prefix + negative_prompt)    \n",
    "    else:\n",
    "        negative_prompt_batch = None\n",
    "\n",
    "    audio_history = None\n",
    "    if all(x is None for x in image_histories):\n",
    "        image_history = None\n",
    "    else:\n",
    "        image_history = image_histories\n",
    "\n",
    "    # For video/audio streams, do we encode the last frame in the input stead of the history\n",
    "    encode_last_frame = input_image is None\n",
    "\n",
    "    # Load video if one is give\n",
    "    audio_history = None\n",
    "    encode_first_frame_as_input = False\n",
    "    if input_video is not None:\n",
    "        if image_histories is not None:\n",
    "            print(gr.Warning(f\"Overriding image histories with video\"))\n",
    "        if isinstance(input_video, str):\n",
    "            image_history, audio_history = load_video(\n",
    "                input_video,\n",
    "                max_frames=IMG_HISTORY_MAX_FRAMES + int(encode_first_frame_as_input),\n",
    "                audio_segment_length=config.AUDIO_SEGMENT_LENGTH,\n",
    "                use_audio=input_audio is None,\n",
    "                target_size=config.IMAGE_HISTORY_INPUT_SIZE,\n",
    "            )\n",
    "        else:\n",
    "            image_history, audio_history = input_video     \n",
    "        if encode_first_frame_as_input:\n",
    "            image_input = image_history[-1]\n",
    "            image_history = image_history[:-1]            \n",
    "            if audio_input is not None:\n",
    "                audio_input = audio_input[-1]\n",
    "                audio_history = audio_history[:-1]\n",
    "    else:\n",
    "        audio_from_video = None\n",
    "\n",
    "    # Load audio\n",
    "    if input_audio is not None:\n",
    "        if audio_history is not None:\n",
    "            print(gr.Warning(f\"Overriding video audio with input\"))\n",
    "        audio_history = load_audio(input_audio)\n",
    "        input_audio = audio_history[-1]\n",
    "        audio_history = audio_history[:-1]\n",
    "    else:\n",
    "        input_audio = None\n",
    "\n",
    "    # Get int random seed\n",
    "    if random_seed.strip() == \"\":\n",
    "        random_seed = np.random.randint(0, 2**31)\n",
    "    else:\n",
    "        try:\n",
    "            random_seed = int(random_seed)\n",
    "        except (TypeError, ValueError) as e:\n",
    "            # Don't crash the whole thing\n",
    "            random_seed = np.random.randint(0, 2**31)\n",
    "            print(gr.Warning(f\"Setting seed failed {e} Using {random_seed}\"))\n",
    "\n",
    "    stats = dict(\n",
    "        modality=output_modality, text=f\"\\\"{input_text}\\\"\", k=top_k, p=top_p, temp=temperature, guidance=guidance_scale,\n",
    "        n_outputs=n_outputs, n_prompt=f\"\\\"{negative_prompt}\\\"\", dec=input_decoding, bbox_annotate_image=bbox_annotate_image,\n",
    "        input_image=input_image.shape if input_image is not None else None, \n",
    "        input_audio=input_audio.shape if input_audio is not None else None, \n",
    "        audio_history=audio_history.shape if audio_history is not None else None, \n",
    "    )\n",
    "    if isinstance(image_history, Sequence):\n",
    "        stats[\"image_history\"] = [None if x is None else x.shape for x in image_history]\n",
    "    else:\n",
    "        stats[\"image_history\"] = None if image_history is None else image_history.shape\n",
    "\n",
    "    print(\"Infer called:\")\n",
    "    print(\", \".join(f\"{k}={v}\" for k, v in stats.items()))\n",
    "\n",
    "    if input_image is not None:\n",
    "        # faster communication after decoding the prediction to gradio demo with lower res image\n",
    "        input_image = _scale_resize(input_image, config.IMAGE_INPUT_SIZE)\n",
    "    \n",
    "    # Input batch\n",
    "    ex, resized_image_input = build_batch(prefix + input_text, input_image, input_audio, image_history, audio_history, True)\n",
    "\n",
    "    # Pick and input length to compress input tokens into\n",
    "    n_nonmasked_tokens = compute_non_masked_input_tokens(ex)\n",
    "    input_len_bucket = n_nonmasked_tokens <= INPUT_LEN_BUCKETS\n",
    "    if any(input_len_bucket):\n",
    "        input_len = INPUT_LEN_BUCKETS[np.argmax(input_len_bucket)]\n",
    "    else:\n",
    "        input_len = None\n",
    "    print(f\"{n_nonmasked_tokens} inputs, using input len bucket of {input_len}\")\n",
    "\n",
    "    num_decodes = n_outputs\n",
    "    if input_decoding == \"Beam\":\n",
    "        greedy = True\n",
    "    else:\n",
    "        greedy = False\n",
    "\n",
    "    if repetition_penalty and not greedy and output_modality == \"text\":\n",
    "        repetition_penalty = 0\n",
    "        print(gr.Info(f\"Repetition penalty not supported for sampling, it will be ignored\"))\n",
    "\n",
    "    # Get the predictions\n",
    "    update_random_seed = gr.Textbox(label=f\"Random seed\", value=random_seed)\n",
    "    output_length = dict(image=1024, audio=512, text=512)[output_modality]\n",
    "\n",
    "    t0 = perf_counter()\n",
    "    print(\"Calling predict...\")\n",
    "    predictions = partitioned_infer_step(\n",
    "        parameters,\n",
    "        ex,\n",
    "        return_all_decodes=True,\n",
    "        num_decodes=num_decodes,\n",
    "        decode_rng=jax.random.PRNGKey(random_seed),\n",
    "        modality=output_modality,\n",
    "        top_k=top_k,\n",
    "        top_p=top_p,\n",
    "        temperature=temperature,\n",
    "        repetition_penalty=repetition_penalty,\n",
    "        length=output_length,\n",
    "        alpha=guidance_scale,\n",
    "        greedy=greedy, \n",
    "        negative_prompt=negative_prompt_batch\n",
    "    )[1]\n",
    "    \n",
    "    # Post-process the predictions to get Gradio outputs\n",
    "    output = []\n",
    "    extra_outputs = []\n",
    "    inputs_that_updated = []\n",
    "    for k, v in predictions.items():\n",
    "        v.block_until_ready()\n",
    "    print(f\"Got results in {perf_counter() - t0:0.4f}, converting to numpy...\")\n",
    "    t0 = perf_counter()\n",
    "    if output_modality == \"image\":\n",
    "        image = np.array(predictions[\"image\"])\n",
    "        if image.shape[0] == 1:\n",
    "            image = image[0]\n",
    "        for i in range(num_decodes):\n",
    "            output.append(np.clip(np.array((image[i] + 1) / 2, dtype=np.float32), 0, 1))\n",
    "    elif output_modality == \"text\":\n",
    "        tokens = np.array(predictions[\"text-tokens\"])\n",
    "        for i in range(num_decodes):\n",
    "            output.append(vocab.decode(tokens.tolist()[0][i]))  # type: ignore\n",
    "\n",
    "        if bbox_annotate_image and input_image is not None:\n",
    "            for i in range(num_decodes):\n",
    "                # anno_img = input_image.copy()\n",
    "                anno_img = resized_image_input.numpy() * 255\n",
    "                decoder_text = output[i]\n",
    "                # 3d bounding box, not visualize labels so far\n",
    "                if decoder_text.startswith(\"<\") or decoder_text.endswith(\">\"):\n",
    "                    decoder_text = \" \" + decoder_text + \" \"\n",
    "\n",
    "                bboxes, class_names = extract_bboxes_from_text(decoder_text, anno_img.shape)\n",
    "                if len(bboxes) > 0:\n",
    "                    anno_img = draw_bboxes(anno_img, bboxes, color=None)\n",
    "                else:\n",
    "                    # 2d keypoints\n",
    "                    points, class_names_points = extract_points_from_text(decoder_text, anno_img.shape)\n",
    "                    if len(points) > 0:\n",
    "                        colors = (\n",
    "                            LOTS_OF_COLORS\n",
    "                            if len(points) <= len(LOTS_OF_COLORS)\n",
    "                            else LOTS_OF_COLORS\n",
    "                            * (len(points) // len(LOTS_OF_COLORS) + 1)\n",
    "                        )\n",
    "                        for p, c in zip(points, colors):\n",
    "                            anno_img = cv2.circle(  # type: ignore\n",
    "                                anno_img, tuple(p.astype(np.int32)[::-1]), 5, c, 2\n",
    "                            )\n",
    "                extra_outputs.append(anno_img / 255.0)\n",
    "        else:\n",
    "            # just to visualize how the input image cropped and resized\n",
    "            extra_outputs = [input_image] * len(output)\n",
    " \n",
    "    elif output_modality == \"audio\":\n",
    "        audio = np.array(predictions[\"audio\"])[0]\n",
    "        for i in range(num_decodes):\n",
    "            if USE_HIFIGAN:\n",
    "                with torch.no_grad():\n",
    "                    # 128 256 1 -> 128 256\n",
    "                    spectrogram = np.array(audio[i] * 3.8312 - 5.0945)[:, :, 0]\n",
    "                    spectrogram = torch.FloatTensor(spectrogram).to(torch_device)\n",
    "                    y_g_hat = hifigan_generator(spectrogram[None, :, :])\n",
    "                    output_audio = y_g_hat.squeeze().cpu().numpy()\n",
    "            else:\n",
    "                spectrogram = np.exp(audio[i] * 3.8312 - 5.0945)[:, :, 0]\n",
    "                output_audio = librosa.feature.inverse.mel_to_audio(  # type: ignore\n",
    "                    spectrogram,\n",
    "                    sr=16000,\n",
    "                    n_fft=1024,\n",
    "                    hop_length=256,\n",
    "                    win_length=None,\n",
    "                    window=\"hann\",\n",
    "                    center=True,\n",
    "                    pad_mode=\"reflect\",\n",
    "                    power=2.0,\n",
    "                    n_iter=32,\n",
    "                )\n",
    "            output.append((config.AUDIO_SAMPLING_RATE, output_audio))\n",
    "    else:\n",
    "        raise NotImplementedError(output_modality)\n",
    "\n",
    "    print(f\"Done in {perf_counter() - t0:0.4f} seconds\")\n",
    "    output += [None]*(NUM_DECODES - num_decodes)\n",
    "    extra_outputs += [None]*(NUM_DECODES - num_decodes)\n",
    "    assert len(output) == NUM_DECODES\n",
    "    return *output, *extra_outputs, *inputs_that_updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b789b999-3690-4bf6-8219-c609762594c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To draw output bounding boxes\n",
    "\n",
    "LOTS_OF_COLORS = [\n",
    "    (255, 0, 0),  # Red\n",
    "    (0, 255, 0),  # Green\n",
    "    (0, 0, 255),  # Blue\n",
    "    (255, 255, 0),  # Yellow\n",
    "    (0, 255, 255),  # Cyan\n",
    "    (255, 0, 255),  # Magenta\n",
    "    (255, 165, 0),  # Orange\n",
    "    (128, 0, 128),  # Purple\n",
    "    (0, 128, 0),  # DarkGreen\n",
    "    (128, 0, 0),  # Maroon\n",
    "    (255, 192, 203),  # Pink\n",
    "    (255, 20, 147),  # DeepPink\n",
    "    (0, 191, 255),  # DeepSkyBlue\n",
    "    (147, 112, 219),  # MediumPurple\n",
    "    (60, 179, 113),  # MediumSeaGreen\n",
    "]\n",
    "\n",
    "HEX_COLORS = [\"#{:02x}{:02x}{:02x}\".format(*_) for _ in LOTS_OF_COLORS]\n",
    "\n",
    "\n",
    "def draw_bboxes(img, bboxes, color=None):\n",
    "    bboxes = np.array(bboxes, np.int32)\n",
    "    img = np.copy(img)\n",
    "\n",
    "    if color is None:\n",
    "        color = LOTS_OF_COLORS\n",
    "        if len(bboxes) > len(color):\n",
    "            color = color * (len(bboxes) // len(color) + 1)\n",
    "    elif isinstance(color, str):\n",
    "        # This will automatically raise Error if rgb cannot be parsed.\n",
    "        color = [ImageColor.getrgb(color)] * len(bboxes)\n",
    "    elif isinstance(color[0], (int, float, np.integer, np.float)):\n",
    "        color = [color] * len(bboxes)\n",
    "    # y1x1y2x2\n",
    "    for (x1, y1, x2, y2), c in zip(bboxes, color):\n",
    "        for x in range(x1, x2):\n",
    "            img[x, y1] = c\n",
    "            img[x, y2 - 1] = c\n",
    "        for y in range(y1, y2):\n",
    "            img[x1, y] = c\n",
    "            img[x2, y] = c\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c129dca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Setup Gradio demo\n",
    "h_shape = config.IMAGE_HISTORY_INPUT_SIZE[:2]\n",
    "\n",
    "negative_prompt_default = {\n",
    "    \"Image\": \"An image of a random picture.\",\n",
    "    \"Audio\": \"A video of a random audio.\"\n",
    "}\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\n",
    "        f\"\"\"\n",
    "        ## Unified-IO-2\n",
    "        #### internel ckpt used: {FULL_CKPT_PATH}\n",
    "        Select the appropriate tab based on the modality of output you want to generate.\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    def make_gradio_input(modality: str, has_bbox_anno: bool = False):\n",
    "        \"\"\"Return tuple[submit button, clear button, inputs, inputs that may be updated (e.g. random seeds)]\"\"\"\n",
    "        input_modality = gr.Textbox(modality, visible=False)\n",
    "        with gr.Row():\n",
    "            input_text = gr.Textbox(placeholder=\"Enter the prompt\", label=\"Input text\")\n",
    "        with gr.Row():\n",
    "            input_image = gr.Image(label=\"Input image\", type=\"numpy\")\n",
    "            input_video = gr.Video(label=\"Input video\")\n",
    "            input_audio = gr.Audio(type=\"filepath\", label=\"Input audio\")\n",
    "\n",
    "        history_rows = IMG_HISTORY_MAX_FRAMES // N_PER_ROW\n",
    "        img_history = []\n",
    "        for i in range(history_rows):\n",
    "            with gr.Row():\n",
    "                img_history.extend(\n",
    "                    [\n",
    "                        gr.Image(label=\"Input image history\", type=\"numpy\")\n",
    "                        for _ in range(N_PER_ROW)\n",
    "                    ]\n",
    "                )\n",
    "        gr.Markdown(\"Hyperparameters:\")\n",
    "        with gr.Row():\n",
    "            random_seed = gr.Textbox(label=\"Random seed\", value=\"\")\n",
    "            top_k = gr.Slider(0, 2000, step=1, label=\"Top-k\", value=0)\n",
    "            top_p = gr.Slider(0, 1, step=0.05, label=\"Top-p\", value=0.9)\n",
    "            temperature = gr.Slider(0, 5, step=0.05, label=\"Temperature\", value=0.9)\n",
    "            repetition_penalty = gr.Slider(0, 10, step=0.2, label=\"Repetition Penalty\", \n",
    "                                           value=1.5 if modality==\"Text\" else 0.0, visible=modality==\"Text\")\n",
    "            if modality != \"Text\":\n",
    "                guidance = gr.Slider(0, 30, step=0.2, label=\"Guidance\", value=10)\n",
    "            else:\n",
    "                guidance = gr.Number(value=0, visible=False)\n",
    "            decoding_param = gr.Dropdown(\n",
    "                [\"Beam\", \"Temperature\"],\n",
    "                label=\"Decoding strategy\",\n",
    "                value=\"Temperature\",\n",
    "            )\n",
    "            n_outputs = gr.Slider(1, 8, step=1, label=\"N-Outputs\", value=8)\n",
    "            if modality != \"Text\":\n",
    "                negative_prompt = gr.Textbox(value=negative_prompt_default.get(modality, \"\"), label=\"Negative prompt\")\n",
    "            else:\n",
    "                negative_prompt = gr.Textbox(value=\"\", label=\"Negative prompt\", visible=False)\n",
    "            if has_bbox_anno:\n",
    "                bbox_annotate_image = gr.Checkbox(\n",
    "                    label=\"Annotate Image\", info=\"e.g. visualize output bounding bbox\"\n",
    "                )\n",
    "            else:\n",
    "                bbox_annotate_image = gr.Number(value=0, visible=False)\n",
    "        with gr.Row():\n",
    "            submit_button = gr.Button(\"Submit\", scale=3)\n",
    "            clear_button = gr.ClearButton(value=\"Clear All\", scale=1)\n",
    "            clear_button.add(\n",
    "                [input_text, input_image, input_audio, input_video, *img_history]\n",
    "            )\n",
    "        return (\n",
    "            submit_button,\n",
    "            clear_button,\n",
    "            [\n",
    "                input_modality,\n",
    "                random_seed,\n",
    "                top_k,\n",
    "                top_p,\n",
    "                temperature,\n",
    "                guidance,\n",
    "                repetition_penalty,\n",
    "                n_outputs,\n",
    "                negative_prompt,\n",
    "                decoding_param,\n",
    "                bbox_annotate_image,\n",
    "                input_text,\n",
    "                input_image,\n",
    "                input_video,\n",
    "                input_audio,\n",
    "                *img_history,\n",
    "            ],\n",
    "            [],\n",
    "        )\n",
    "\n",
    "    def make_gradio_output(\n",
    "        modality, has_bbox_anno=False, updated_inputs: Optional[list] = None\n",
    "    ):\n",
    "        output_list = []\n",
    "        if modality == \"Text\":\n",
    "            for i in range(N_ROWS):\n",
    "                with gr.Row():\n",
    "                    output_list.extend(\n",
    "                        [gr.Textbox(label=\"Output\") for _ in range(N_PER_ROW)]\n",
    "                    )\n",
    "            if has_bbox_anno:\n",
    "                for i in range(N_ROWS):\n",
    "                    with gr.Row():\n",
    "                        output_list.extend(\n",
    "                            [gr.Image(label=\"Annotation\") for _ in range(N_PER_ROW)])\n",
    "        elif modality == \"Image\":\n",
    "            for i in range(N_ROWS):\n",
    "                with gr.Row():\n",
    "                    output_list.extend(\n",
    "                        [gr.Image(label=\"Output\") for _ in range(N_PER_ROW)])\n",
    "        elif modality == \"Audio\":\n",
    "            for i in range(N_ROWS):\n",
    "                with gr.Row():\n",
    "                    output_list.extend(\n",
    "                        [gr.Audio(label=\"Output\") for _ in range(N_PER_ROW)])\n",
    "        if updated_inputs is not None:\n",
    "            output_list.extend(updated_inputs)\n",
    "        return output_list\n",
    "\n",
    "    with gr.Tab(\"Text Generation\"):\n",
    "        text_button, text_clear, text_inputs, text_updated_inputs = make_gradio_input(\n",
    "            \"Text\", has_bbox_anno=True\n",
    "        )\n",
    "        text_outputs = make_gradio_output(\n",
    "            \"Text\", has_bbox_anno=True, updated_inputs=text_updated_inputs\n",
    "        )\n",
    "        text_clear.add([_ for _ in text_outputs if _ not in text_updated_inputs])\n",
    "\n",
    "    with gr.Tab(\"Image Generation\"):\n",
    "        (\n",
    "            image_button,\n",
    "            image_clear,\n",
    "            image_inputs,\n",
    "            image_updated_inputs,\n",
    "        ) = make_gradio_input(\"Image\")\n",
    "        image_outputs = make_gradio_output(\n",
    "            \"Image\", has_bbox_anno=False, updated_inputs=image_updated_inputs\n",
    "        )\n",
    "        image_clear.add([_ for _ in image_outputs if _ not in image_updated_inputs])\n",
    "\n",
    "    with gr.Tab(\"Audio Generation\"):\n",
    "        (\n",
    "            audio_button,\n",
    "            audio_clear,\n",
    "            audio_inputs,\n",
    "            audio_updated_inputs,\n",
    "        ) = make_gradio_input(\"Audio\")\n",
    "        audio_outputs = make_gradio_output(\n",
    "            \"Audio\", has_bbox_anno=False, updated_inputs=audio_updated_inputs\n",
    "        )\n",
    "        audio_clear.add([_ for _ in audio_outputs if _ not in audio_updated_inputs])\n",
    "\n",
    "    text_button.click(run_inference, inputs=text_inputs, outputs=text_outputs)\n",
    "    image_button.click(run_inference, inputs=image_inputs, outputs=image_outputs)\n",
    "    audio_button.click(run_inference, inputs=audio_inputs, outputs=audio_outputs)\n",
    "\n",
    "demo.queue().launch(share=True, show_error=True, max_threads=os.cpu_count() - 10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
